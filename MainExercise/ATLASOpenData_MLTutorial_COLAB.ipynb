{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pdg8zx3YYMRt"
      },
      "source": [
        "# ATLAS OpenData Machine Learning Tutorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/els285/ML-ATLASOpenData/blob/main/MainExercise/ATLASOpenData_MLTutorial_COLAB.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/els285/ML-ATLASOpenData/main?filepath=MainExercise%2FATLASOpenData_MLTutorial_COLAB.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwiE525yYMRu"
      },
      "source": [
        "This tutorial takes you through how to download ATLAS OpenData, convert the data into a format suitable for building machine learning models, and then build such a model: a \"simple\" deep neural network for classifying two processes as signal and background.\n",
        "\n",
        "The signal process we consider is the production of a Higgs boson through gluon-gluon fusion, where the Higgs then decays to two W bosons which both decay leptonically.\n",
        "The background process is top-antitop quark pair production, with both tops decaying through two intermediate W bosons which also decay leptonically.\n",
        "Thus the final state consists of two charged leptons, some missing transverse energy, and some jets.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/els285/Intro2NN4Physics/main/images/ATLAS-FUELED-AI-VERTICAL.png\" width=\"300\">\n",
        "\n",
        "\n",
        "## Aims of this tutorial\n",
        "* Pipeline for turning ATLAS OpenData for Education ROOT files into a format for machine learning\n",
        "* Building and training a simple DNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDcHHYC4BDn7"
      },
      "source": [
        "# Part 0 - Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP_hByeKYlhA"
      },
      "source": [
        "Like the other tutorials, we will make use of the excellent `atlasopenmagic` package. Note that if you're running this in SWAN, there should be an extra cell before. You may already have the required packages installed: we need `numpy`,`awkward` and `uproot` for the pre-processing, as well as `torch` and `sklearn` for the machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FCC1h8gXYhy9",
        "outputId": "946f2126-6ad8-4790-af07-0cbbfedcd487"
      },
      "outputs": [],
      "source": [
        "#install required packages\n",
        "import sys\n",
        "%pip install atlasopenmagic\n",
        "from atlasopenmagic import install_from_environment\n",
        "install_from_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "s466F2LGaILx"
      },
      "outputs": [],
      "source": [
        "# Check that numpy, akward, uproot, torch and sklearn are installed\n",
        "import numpy as np\n",
        "import awkward as ak\n",
        "import uproot\n",
        "import torch\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIYg_yq_AtM4"
      },
      "source": [
        "# Part 1 - Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gximVuTuYMRv"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8OXqW63uPjq"
      },
      "source": [
        "We access the data (which is actually simulated data) through `atlasopenmagic`. Hopefully you've learned a bit about what the below cell does. We're using the newest Outreach for Education datasets at 13 TeV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AZYnG71Yb5O",
        "outputId": "43c0bd27-aefa-43c4-c383-899f37b9afd9"
      },
      "outputs": [],
      "source": [
        "import atlasopenmagic as atom\n",
        "atom.available_releases()\n",
        "atom.set_release('2025e-13tev-beta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYmr9TeFucXx"
      },
      "source": [
        "The cell below provides the URL for the signal and background files. We are using the `2to4lep` skim. This skim is a slice of the data which contains between 2 and 4 leptons - and we'll apply further selections later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "N5o8kONpuN20"
      },
      "outputs": [],
      "source": [
        "signal_file_list     = atom.get_urls('346802', skim=\"2to4lep\", protocol='https', cache=True)\n",
        "background_file_list = atom.get_urls('411234', skim=\"2to4lep\", protocol='https', cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTfdwWpEYMRz"
      },
      "source": [
        "## Data pre-processing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqEur1m7YMRz"
      },
      "source": [
        "The above cells are all `atlasopenmagic` functionality to get our data. We will now use the scikit-HEP software stack to:\n",
        "* Load the data (it's in the form of ROOT files)\n",
        "* Perform some basic selections to filter the data\n",
        "* Convert directly to PyTorch tensors\n",
        "\n",
        "We will perform the operations on the signal file, and then you can repeat these operations on the background file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hA8E-YgpYMRy"
      },
      "outputs": [],
      "source": [
        "signal_file = uproot.open(signal_file_list[0])\n",
        "signal_tree = signal_file[\"analysis\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsjdvC3YMRz"
      },
      "source": [
        "We will extract only jet, lepton and MET information. The below creates an awkward array with all these fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "45ADvDYxYMRz"
      },
      "outputs": [],
      "source": [
        "signal_array = signal_tree.arrays([\"lep_pt\", \"lep_eta\", \"lep_phi\", \"lep_e\",\n",
        "                                  \"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_e\",\n",
        "                                  \"met\", \"met_phi\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV5sSD-LYMRz"
      },
      "source": [
        "We are now in a position to apply some selections to the data. Firstly, we will stipulate that we want at least 2 jets in each event, creating a new array with this selection applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GCyE0LcgYMRz"
      },
      "outputs": [],
      "source": [
        "# Creates a mask which is True for events with at least 2 jets\n",
        "mask_2plus_jets = ak.num(signal_array[\"jet_pt\"]) >= 2\n",
        "\n",
        "# Apply the mask to select only those events\n",
        "signal_filtered_array = signal_array[mask_2plus_jets]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUzDogajYMR0"
      },
      "source": [
        "We used the \"2to4lep\" skim but we want to look specifically in the channel that has exactly two charged leptons.\n",
        "Here we apply a more complex selection, where we filter on lepton pT, and then require that two leptons remain after this.\n",
        "\n",
        "(This is an example of an object-level selection followed by an event-level selection, and we have to apply the cuts in this order. Do you see why?)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xfDVgWZFYMR0"
      },
      "outputs": [],
      "source": [
        "# Construct mask for >\n",
        "lep_10GEV_mask = signal_filtered_array[\"lep_pt\"] >= 10\n",
        "\n",
        "# Apply the mask to select only leptons with pt >= 10 GeV. We over-write the columns in the signal_filtered_array\n",
        "signal_filtered_array[\"lep_pt\"] = signal_filtered_array[\"lep_pt\"][lep_10GEV_mask]\n",
        "signal_filtered_array[\"lep_eta\"] = signal_filtered_array[\"lep_eta\"][lep_10GEV_mask]\n",
        "signal_filtered_array[\"lep_phi\"] = signal_filtered_array[\"lep_phi\"][lep_10GEV_mask]\n",
        "signal_filtered_array[\"lep_e\"] = signal_filtered_array[\"lep_e\"][lep_10GEV_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "go0ks_WGYMR0"
      },
      "outputs": [],
      "source": [
        "# Build the mask to specify exactly 2 leptons\n",
        "mask_exactly2leptons = ak.num(signal_filtered_array[\"lep_pt\"]) == 2\n",
        "\n",
        "# Apply that selection\n",
        "final_signal_array = signal_filtered_array[mask_exactly2leptons]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAzcNjbqwrBE"
      },
      "source": [
        "We can see how the total number of events changes as a function of the selections we've applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6MY8iCVYMR0",
        "outputId": "e4d54abb-c58d-49b9-fc68-0d2c81d4be1f"
      },
      "outputs": [],
      "source": [
        "print(\"Number of events in the original signal dataset:\", len(signal_array))\n",
        "print(\"Number of events with at least 2 jets:\", len(signal_filtered_array))\n",
        "print(\"Number of events with at least 2 jets and 2 leptons with pt >= 10 GeV:\", len(final_signal_array))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2z0skCLA3In"
      },
      "source": [
        "# Part 2 - Moving to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeDaFjvnYMR0"
      },
      "source": [
        "## Converting to PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybEYL5l0YMR0"
      },
      "source": [
        "`PyTorch` is a Python library for building machine learning models, everything from simple deep neural networks to advanced state-of-the-art models!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEC2_l_FYMR0"
      },
      "source": [
        "For this exercise, we will turn everything into a torch tensor.\n",
        "The tensor will be two-dimensional and have the shape `number_of_events x number_of_branches`.\n",
        "This is because we are going to train a deep neural network, which will require each data instance (each event) to be a 1D vector of values.\n",
        "\n",
        " Let's practice just by taking the kinematics of the zeroth lepton in each event, converting these into individual tensors and then stacking them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lYL9T5iRYMR0"
      },
      "outputs": [],
      "source": [
        "# Import the torch module\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fz5lz6hYMR0"
      },
      "source": [
        "Let's look at how to convert the zeroth lepton kinematics to individual torch tensors, then combine in to one big tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tR3vtjRHYMR0"
      },
      "outputs": [],
      "source": [
        "# Convert the lepton kinematic variables of the zeroth lepton in each event to PyTorch tensors.\n",
        "# The [:,0] indexing selects the zeroth lepton from each event.\n",
        "# We also define the datatype for each array - this makes things faster\n",
        "\n",
        "lepton0_pt_tensor  = torch.tensor(final_signal_array[\"lep_pt\"][:,0] , dtype=torch.float32)\n",
        "lepton0_eta_tensor = torch.tensor(final_signal_array[\"lep_eta\"][:,0], dtype=torch.float32)\n",
        "lepton0_phi_tensor = torch.tensor(final_signal_array[\"lep_phi\"][:,0], dtype=torch.float32)\n",
        "lepton0_e_tensor   = torch.tensor(final_signal_array[\"lep_e\"][:,0]  , dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "at8NYTy3YMR0"
      },
      "outputs": [],
      "source": [
        "# Stack the tensors into a single tensor\n",
        "signal_tensor = torch.stack([lepton0_pt_tensor,\n",
        "             lepton0_eta_tensor,\n",
        "             lepton0_phi_tensor,\n",
        "             lepton0_e_tensor], dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLikJNunYMR0"
      },
      "source": [
        "We now have an object that we can use to train machine learning tools!\n",
        "\n",
        "I have written this in a verbose way to hopefully make it clear what is happening in the simple case where the consider only one of the leptons.\n",
        "\n",
        "Let's extend this to converting all the data in our signal_array into torch. The following code looks a complicated but simply creates a tensor of zeros and then fills each column with the data from our signal array.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZhDtb7_2YMR1"
      },
      "outputs": [],
      "source": [
        "signal_tensor = torch.zeros(len(final_signal_array), 18)\n",
        "\n",
        "for i,branch in enumerate([\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_e\"]):\n",
        "    signal_tensor[:,i]    = torch.tensor(final_signal_array[branch][:,0], dtype=torch.float32)\n",
        "    signal_tensor[:,i+4]  = torch.tensor(final_signal_array[branch][:,1], dtype=torch.float32)\n",
        "\n",
        "for i,branch in enumerate([\"jet_pt\",\"jet_eta\",\"jet_phi\",\"jet_e\"]):\n",
        "    signal_tensor[:,i+8]  = torch.tensor(final_signal_array[branch][:,0], dtype=torch.float32)\n",
        "    signal_tensor[:,i+12] = torch.tensor(final_signal_array[branch][:,1], dtype=torch.float32)\n",
        "\n",
        "signal_tensor[:,16] = torch.tensor(final_signal_array[\"met\"] , dtype=torch.float32)\n",
        "signal_tensor[:,17] = torch.tensor(final_signal_array[\"met_phi\"], dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfaWRr8sYMR1"
      },
      "source": [
        "Now we need to do the same thing for the background dataset. Please copy the above steps to load, apply selections and build a tensor called `background_tensor`. Note that the background dataset has more events, so to speed things up you can cut this at 150k events.\n",
        "\n",
        "If you want an easy life, just use the function below..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "6nEHIe3Bpnlx"
      },
      "outputs": [],
      "source": [
        "def prepare_data(file_path):\n",
        "\n",
        "  \"\"\"\n",
        "  This function takes as argument the path to a ROOT file;\n",
        "  Applies the selections and transformations outlined above;\n",
        "  Converts the output to a PyTorch tensor and returns it.\n",
        "  \"\"\"\n",
        "\n",
        "  file = uproot.open(file_path)\n",
        "  tree = file[\"analysis\"]\n",
        "\n",
        "  array = tree.arrays([\"lep_pt\", \"lep_eta\", \"lep_phi\", \"lep_e\",\n",
        "                                  \"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_e\",\n",
        "                                  \"met\", \"met_phi\"])\n",
        "\n",
        "  # Creates a mask which is True for events with at least 2 jets\n",
        "  mask_2plus_jets = ak.num(array[\"jet_pt\"]) >= 2\n",
        "\n",
        "  # Apply the mask to select only those events\n",
        "  filtered_array = array[mask_2plus_jets]\n",
        "\n",
        "  # Construct mask for >\n",
        "  lep_10GEV_mask = filtered_array[\"lep_pt\"] >= 10\n",
        "\n",
        "  # Apply the mask to select only leptons with pt >= 10 GeV. We over-write the columns in the signal_filtered_array\n",
        "  filtered_array[\"lep_pt\"]  = filtered_array[\"lep_pt\"][lep_10GEV_mask]\n",
        "  filtered_array[\"lep_eta\"] = filtered_array[\"lep_eta\"][lep_10GEV_mask]\n",
        "  filtered_array[\"lep_phi\"] = filtered_array[\"lep_phi\"][lep_10GEV_mask]\n",
        "  filtered_array[\"lep_e\"]   = filtered_array[\"lep_e\"][lep_10GEV_mask]\n",
        "\n",
        "  #\n",
        "  # Build the mask to specify exactly 2 leptons\n",
        "  mask_exactly2leptons = ak.num(filtered_array[\"lep_pt\"]) == 2\n",
        "\n",
        "  # Apply that selection\n",
        "  final_array = filtered_array[mask_exactly2leptons]\n",
        "\n",
        "  data_tensor = torch.zeros(len(final_array), 18)\n",
        "\n",
        "  for i,branch in enumerate([\"lep_pt\",\"lep_eta\",\"lep_phi\",\"lep_e\"]):\n",
        "      data_tensor[:,i] = torch.tensor(final_array[branch][:,0], dtype=torch.float32)\n",
        "      data_tensor[:,i+4] = torch.tensor(final_array[branch][:,1], dtype=torch.float32)\n",
        "\n",
        "  for i,branch in enumerate([\"jet_pt\",\"jet_eta\",\"jet_phi\",\"jet_e\"]):\n",
        "      data_tensor[:,i+8] = torch.tensor(final_array[branch][:,0], dtype=torch.float32)\n",
        "      data_tensor[:,i+12] = torch.tensor(final_array[branch][:,1], dtype=torch.float32)\n",
        "\n",
        "  data_tensor[:,16] = torch.tensor(final_array[\"met\"] , dtype=torch.float32)\n",
        "  data_tensor[:,17] = torch.tensor(final_array[\"met_phi\"], dtype=torch.float32)\n",
        "\n",
        "  return data_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "bok8iVPbYMR1"
      },
      "outputs": [],
      "source": [
        "# This can sometimes tak a couple of minutes to run depending on how Colab is feeling\n",
        "signal_tensor = prepare_data(signal_file_list[0])\n",
        "background_tensor = prepare_data(background_file_list[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EYnbikI_nU4"
      },
      "source": [
        "Let's take 100k events from each:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "0BTAyg2xkq0j"
      },
      "outputs": [],
      "source": [
        "signal_tensor = signal_tensor[:100000]\n",
        "background_tensor = background_tensor[:100000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX6-DPEBYMR7"
      },
      "source": [
        "## Preparing the data for machine learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8G9zCWzhupf"
      },
      "source": [
        "There's a couple more pre-processing steps we have to do...\n",
        "Firstly, we need to combine our signal and background data together into one dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "QSoiolfsYMR8"
      },
      "outputs": [],
      "source": [
        "# Combining signal and data tensors through concatenation\n",
        "input_tensor = torch.cat([signal_tensor,background_tensor], dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yQrkjYAh_0B"
      },
      "source": [
        "This constitutes the data which will go into the model. Supervised learning also requires a target. We are trying to teach the model that the events in the `signal_tensor` correspond to signal, and those in the `background_tensor` correspond to background. To do this, we will assign the target value for the signal events a 1, and the background events 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "OF9Lqor1iEL4"
      },
      "outputs": [],
      "source": [
        "# Defining the signal and background targets as tensors of 1s and 0s respectively\n",
        "signal_target     = torch.ones(len(signal_tensor),      dtype=torch.float32)\n",
        "background_target = torch.zeros(len(background_tensor), dtype=torch.float32)\n",
        "\n",
        "# Combining together in the same order as the inputs\n",
        "target_tensor = torch.cat([signal_target,background_target], dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC06YixYYMR7"
      },
      "source": [
        "We have our data in the correct format, but robust machine learning requires some additional formatting.\n",
        "First we want to split the data into training, testing and validation samples.\n",
        "The training sample is the data which we'll use to train our model.\n",
        "We can't then use the same data to evaluate the model's performance, because it has already seen this data - we need to know how it performs on unseen data!\n",
        "We therefore create a separate testing dataset and evaluate how the model performs on this data.\n",
        "I like to use the scikit-learn functionality for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "G0XJtuEQYMR8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(input_tensor, test_size=0.2, random_state=42)\n",
        "Y_train, Y_test = train_test_split(target_tensor, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDfeIE2rYMR8"
      },
      "source": [
        "You will notice that some features of the data, like `jet_e` are hundreds of times larger than other features like `jet_phi`. This can create problems for training models, so it is advisable to scale the data prior to training. There are many different transforms you can try, with a goal to get each feature to have roughly equivalent scale.\n",
        "\n",
        "One can use `sklearn.preprocessing.standard_scaler` for this, but I have explicitly written a transformation where we subtract the mean and divide by the standard deviation. This is called a Z-transform - but other transformations are avaialble ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "4BM6CP2dYMR8"
      },
      "outputs": [],
      "source": [
        "# Compute the mean and standard deviation\n",
        "mean = X_train.mean(dim=0, keepdim=True)\n",
        "std = X_train.std(dim=0, keepdim=True)\n",
        "\n",
        "# Transform step: Apply the standardization formula\n",
        "X_train_scaled = (X_train - mean) / std\n",
        "X_test_scaled = (X_test - mean) / std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_RAbISBYMR8"
      },
      "source": [
        "Finally, machine learning algorithms achieve best training performance when they run on GPUs. You can move torch tensors to whichever \"device\" you wish through the syntax below. For this example we will stick to using the CPU. If you are using Google Colab, change the runtime to ask for a GPU and then change device to \"cuda\". You should see a performance increase when it comes to training the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "f2VEsNaZrEiI"
      },
      "outputs": [],
      "source": [
        "device = \"cpu\"\n",
        "X_train_scaled = X_train_scaled.to(device)\n",
        "X_test_scaled = X_test_scaled.to(device)\n",
        "\n",
        "Y_train = Y_train.to(device)\n",
        "Y_test = Y_test.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOs2-n1jYMR8"
      },
      "source": [
        "# Part 3 -  Building our DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clMNXk9QYMR8"
      },
      "source": [
        "In PyTorch, we build a forward model by combining PyTorch operations. Such operations are part of the `nn` submodule. The syntax is in general the same: first you define the operation, then you call it as a function which takes the data as input.\n",
        "\n",
        "`nn.Linear` layers multiply our input data by some learnable weight matrix, and then add some learnable bias vector. This constitutes an affine transformation of the data. We can use these layers to map out data to higher and lower-dimensional latent spaces. For example, the below operations maps the 18 input features into a feature space of dimension 128."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sabGPzWeYMR8",
        "outputId": "9e70c1fa-b1c4-4e72-96d9-ccd0c3ab611e"
      },
      "outputs": [],
      "source": [
        "# For example, applying an affine transformation of multiplicating by a weight matrix and addition of a bias term (we rather sloppily call these operations liner operations)\n",
        "from torch import nn\n",
        "\n",
        "affine_layer = nn.Linear(18,128)\n",
        "affine_layer(X_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2alKPZ87u-UN"
      },
      "source": [
        "Activation functions embue the model with the non-linearity required to model complex non-linear functions. There are many activation functions available. Unlike linear layers, these don't change the dimensionality of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3zm43SFYMR8",
        "outputId": "15462bb7-74d1-44bd-ddc5-4bb6691490bd"
      },
      "outputs": [],
      "source": [
        "# Application of some non-linear \"activation function\"\n",
        "\n",
        "activ_func = nn.Sigmoid()\n",
        "activ_func(X_train_scaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbuEmCWhsckM"
      },
      "source": [
        "To build a DNN we interleave Linear layers with activation functions, uisng the `nn.Sequential` syntax to define a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "kIh2d7pwYMR8"
      },
      "outputs": [],
      "source": [
        "# Using nn.Sequential\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(18, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid(),\n",
        ")\n",
        "\n",
        "# Our model should live on the same device as our tensors\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_93D5ZniwVdF"
      },
      "source": [
        "The model (or forward model) is just one component of our full DNN algorithm. It only tells the algorithm what type operations to apply to the input data. The key to machine learning is that we iteratively update the parameters of these operations in a smart way, and for that, we need a loss function (which I call here `criterion`) and an optimisation algorithm which can update the model parameters based on the calculated loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "id": "yQMuxU2yYMR9"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "import torch.optim as optim\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK4u4oesw7Bz"
      },
      "source": [
        "Now we really do have everything we need: our input data, our forward model, our loss and our optimiser. All that remains is to train the algorithm! To do this, we pass the input data to the forward model, compute the loss, and update the model parameters accordingly. Then we repeat this process for `N_epochs`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzUkGONAYMR9",
        "outputId": "f4d06b32-0379-4cf6-ca61-a699e544392f"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "\n",
        "N_epochs = 75\n",
        "\n",
        "for epoch in range(N_epochs):\n",
        "\n",
        "    model.train()\n",
        "    # tell the optimizer to begin an optimization step\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    # use the model as a prediction function: features â†’ predictions\n",
        "    predictions = model(X_train_scaled)\n",
        "\n",
        "    # compute the loss between these predictions and the intended targets\n",
        "    loss = criterion(predictions.squeeze(), Y_train)\n",
        "\n",
        "    # Compute the gradients of the loss wrt the trainable parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the model parameters based on the gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Purely for monitoring and visualisation: keeping track of the losses\n",
        "    train_losses.append(loss.item())\n",
        "    print(f'Epoch [{epoch + 1}/{N_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehi58ZKMyC5D"
      },
      "source": [
        "If everything has been set up correctly, we should see that the loss goes down but has not yet converged to a stable minimum. Let's visualise this in a simple plot (don't worry too much about the plotting syntax)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "xZr8YliPYMR9",
        "outputId": "a0928d53-d71b-4412-f0e2-074cb235701d"
      },
      "outputs": [],
      "source": [
        "# Plot loss function for training and validation sets\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses, label='Train Loss', color='red')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Validation Loss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAzYxlhuySfA"
      },
      "source": [
        "Not bad, but we'd like to see the loss plateau! Have a go at changing some model parameters and see a) if you can get the loss to flatten out, and b) how quickly you can get the model to flatten out. You can adjust things like:\n",
        "* The number of epochs\n",
        "* The number of linear layers in the DNN\n",
        "* The type of activation functions - but the last one should be a sigmoid!\n",
        "* The learning rate and the optimiser (in particular this might benefit from a larger learning rate)\n",
        "\n",
        "It shouldn't be hard to find a reasonable model, since our data are pretty easily separable into two classes!\n",
        "\n",
        "Once you're happy your model has converged, we can go on to defining some metrics to evaluate our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0-OiL3XYMR9"
      },
      "source": [
        "# Part 4 - Evaluating our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbWmTkkuYMR9"
      },
      "source": [
        "Our DNN is now trained, which means that the internal parameters (weights and biases) are hopefully set to \"optimal\" values (where optimality is achieved by gradient descent in the loss space).\n",
        "We now evaluate the model simply by passing the testing data through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZTbFBGDYMR9",
        "outputId": "cc35c67d-cacd-4299-88eb-ac950fa822ce"
      },
      "outputs": [],
      "source": [
        "# Model evaluation is simple: call the model\n",
        "# We detach the array\n",
        "y_pred = model(X_test_scaled).detach()\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nEi4Ng0YMR9"
      },
      "source": [
        "The tensor `y_pred` gives a continuous score, not a binary decision. For a well trained binary classifier, these scores should show good separation, which can be seen through a simple plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "-to4JyQ2YMR9",
        "outputId": "b33544ba-fd35-4c4f-ea14-d0422bc1b7d6"
      },
      "outputs": [],
      "source": [
        "# Plot the scores\n",
        "plt.hist(y_pred,bins=40)\n",
        "plt.xlabel('Model output score')\n",
        "plt.ylabel('Events')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxa47RTxYMR9"
      },
      "source": [
        "There data exhibit two  peaks  close to 0 and 1, indicating that the DNN can separate the data into two classes reasonably well.\n",
        "\n",
        "Let's now evaluate whether the model is predicting the classes correctly. This is an arbitrary choice on our part: we must define some threshold value for the model output, above which we consider events to be classed as signal, and below which as background. Based on the above plot, the standard value of 0.5 is appropriate e.g. if the model returns a score >0.5, we shall class this as a signal score, otherwise it's background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "yNEn40PzYMR9"
      },
      "outputs": [],
      "source": [
        "threshold = 0.5\n",
        "y_pred_labels = (y_pred >= threshold).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lckSwW5YMR-"
      },
      "source": [
        "Scikit-learn is useful for providing metrics for evaluating model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n34xKxsuYMR-",
        "outputId": "5fe1fe1b-a8c6-43e0-9faf-8eb9adca2d28"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(Y_test, y_pred_labels)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "g-p_uSF4YMR-",
        "outputId": "b9682588-d101-477b-8419-4b70f0e7b08d"
      },
      "outputs": [],
      "source": [
        "# Plotting the model outputs for signal and background separately\n",
        "signal_scores = y_pred[Y_test == 1].flatten()\n",
        "background_scores = y_pred[Y_test == 0].flatten()\n",
        "plt.hist([signal_scores,background_scores], bins=40, histtype='step', label=['Signal','Background'])\n",
        "plt.legend()\n",
        "plt.xlabel('Model output score')\n",
        "plt.ylabel('Events')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "z0tlJNdiYMR-",
        "outputId": "cca7fe81-2133-4504-bc3e-acb817e646d7"
      },
      "outputs": [],
      "source": [
        "# ROC curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, thresholds = roc_curve(Y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l49iLR5Hp4ab"
      },
      "source": [
        "Now you know how to build and evaluate you a DNN. Not that hard is it? Perhaps the trickiest part is formulating your data into the correct format.\n",
        "\n",
        "Now, can you get a model which delivers an AUC over 0.95????\n",
        "\n",
        "This is the end of the nominal tutorial. What follows are some additional exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqpULlZsYMR-"
      },
      "source": [
        "#  Bonus 1 - More complex data preparation\n",
        "\n",
        "We have looked at all stages of the ML pipeline in one notebook. It is good practice to split our ML pipeline up into separate Python scripts: one to prepare the data, one to load it into the PyTorch model and train, one to evaluate, and one to study the results. Part of this is then saving our processed data in a format which is suitable for then reading in to PyTorch i.e. saving the data to HDF5 format (industry standard) after applying the selections, and then loading these HDF5 files into torch tensors. The following section shows how to do this..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16lj5fEHYMR-"
      },
      "source": [
        "## Saving to HDF5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvQXbhPMYMR-"
      },
      "source": [
        "We will convert the `signal_array` to a structured numpy array and then use the `h5py` module to save this. `numpy` - as you may already know - is used for array-based programming similarly to `torch`, but more widely used and not optimised for machine learning use-cases (`torch` is designed to be used on the GPU).\n",
        "\n",
        "Let's look at the simple single lepton example..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {
        "id": "p_o6asAUYMR-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# First we create a unstructured numpy array from the awkward array, by converting each branch to a numpy array and stacking them column-wise\n",
        "\n",
        "unstructured_numpy_array = np.column_stack([\n",
        "    final_signal_array['lep_pt'][:,0].to_numpy(),\n",
        "    final_signal_array['lep_eta'][:,0].to_numpy(),\n",
        "    final_signal_array['lep_phi'][:,0].to_numpy(),\n",
        "    final_signal_array['lep_e'][:,0].to_numpy()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {
        "id": "MC-hkyvvYMR-"
      },
      "outputs": [],
      "source": [
        "# Now we define the data type for the structured array\n",
        "structured_data_type = np.dtype([\n",
        "    ('lep_pt', np.float32),\n",
        "    ('lep_eta', np.float32),\n",
        "    ('lep_phi', np.float32),\n",
        "    ('lep_e', np.float32)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "7bxq7IHWYMR-"
      },
      "outputs": [],
      "source": [
        "# Finally, we convert the unstructured numpy array to a structured numpy array using built-in numpy functionality\n",
        "from numpy.lib import recfunctions as rf\n",
        "structured_numpy_array = rf.unstructured_to_structured(\n",
        "    unstructured_numpy_array,\n",
        "    dtype=structured_data_type\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "pXQxDOkaYMR-"
      },
      "outputs": [],
      "source": [
        "# Now we can save this as an HDF5 file using h5py\n",
        "import h5py\n",
        "with h5py.File('signal_data.h5', 'w') as h5file:\n",
        "    h5file.create_dataset('signal_dataset', data=structured_numpy_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL_XpAtxYMR-"
      },
      "source": [
        "### Re-loading the data for HDF5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "LrwLCXC1YMR-"
      },
      "outputs": [],
      "source": [
        "# Reading the file\n",
        "with h5py.File('signal_data.h5', 'r') as h5file:\n",
        "    loaded_data = h5file['signal_dataset'][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "ATCn6cgJYMR_"
      },
      "outputs": [],
      "source": [
        "# Conver the loaded data to an unstructured numpy array using numpy.lib.recfunctions\n",
        "loaded_numpy_array = rf.structured_to_unstructured(loaded_data)\n",
        "# Convert to a torch tensor\n",
        "input_tensor = torch.tensor(loaded_numpy_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Omt9MxtZskdN",
        "outputId": "1f70def1-5d9a-469e-aa80-cdf4c07d5d06"
      },
      "outputs": [],
      "source": [
        "input_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUJ5JS9OYMR_"
      },
      "source": [
        "## Padding\n",
        "\n",
        "You may have noticed that our ATLAS OpenData has a \"ragged\" structure i.e. fields can have different lengths in different events.\n",
        "For example, the number of jets varies event-to-event.\n",
        "This is of course a key feature of particle physics data, and a key reason why we use ROOT and the awkward framework.\n",
        "\n",
        "In general, machine learning frameworks expect non-ragged inputs: our tensors should be rectangular.\n",
        "We got round this above by looking at events with exactly two leptons and two jets, hence every event had the same amount of features (18 including the two MET variables).\n",
        "\n",
        "In the more general case, we can **pad** arrays with dummy values. An example is shown below where look at the first four jets in each event, and if an event has less than four events, we \"pad\" these entries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xpr5j06s6A0"
      },
      "source": [
        "### Creating padded data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8wppZqLs8mZ"
      },
      "source": [
        "The following cells extract the jet data and pad such that each event has exactl 4 jets. We set the padded jet kinematics to be something unphysical: -99."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "zz972d62YMR_"
      },
      "outputs": [],
      "source": [
        "# Loading the jet data\n",
        "jet_array = signal_tree.arrays([\"jet_pt\",\"jet_eta\",\"jet_phi\",\"jet_e\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "J3WGL9VgYMR_"
      },
      "outputs": [],
      "source": [
        "# Padding the arrays using awkward's pad_none, which pads missing entries with Python's None class\n",
        "padded_jet_array = ak.pad_none(jet_array,4,clip=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "id": "FX2su2PfYMR_"
      },
      "outputs": [],
      "source": [
        "# Setting these None values to unphysical float values\n",
        "padded_jet_array = ak.fill_none(padded_jet_array,-99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of4VuuFRtarN"
      },
      "source": [
        "Now we map this object to a structured numpy array, analogously to before. We define a specific dtype, create an array of zeros, and then set the values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "gSEnC0qAYMR_"
      },
      "outputs": [],
      "source": [
        "# Creating the data_type\n",
        "import numpy as np\n",
        "structured_data_type = np.dtype([\n",
        "    ('jet_pt', np.float32),\n",
        "    ('jet_eta', np.float32),\n",
        "    ('jet_phi', np.float32),\n",
        "    ('jet_e', np.float32)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "h_5HOX6mYMR_"
      },
      "outputs": [],
      "source": [
        "# Creating a placeholder numpy array of zeros. This array is of shape Nevents x 4, but each entry will have a structure based on the structured_data-type\n",
        "jet_data = np.zeros((len(padded_jet_array),4),dtype=structured_data_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "Ro5q-m11YMR_"
      },
      "outputs": [],
      "source": [
        "jet_data[\"jet_pt\"]   = padded_jet_array[\"jet_pt\"].to_numpy()\n",
        "jet_data[\"jet_eta\"]  = padded_jet_array[\"jet_eta\"].to_numpy()\n",
        "jet_data[\"jet_phi\"]  = padded_jet_array[\"jet_phi\"].to_numpy()\n",
        "jet_data[\"jet_e\"]    = padded_jet_array[\"jet_e\"].to_numpy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe99QykGYMR_"
      },
      "source": [
        "### Reloading the padded data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3joWV7lYMR_"
      },
      "source": [
        "Re-loading from HDF5 and converting to pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gdo5rGaFYMR_"
      },
      "outputs": [],
      "source": [
        "# Again using numpy inbuilt-in functionality - this should be encouraged\n",
        "from numpy.lib.recfunctions import structured_to_unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvdDzJOdYMR_"
      },
      "outputs": [],
      "source": [
        "unstructured_jet_data = structured_to_unstructured(jet_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2e7Qdb6YMR_"
      },
      "source": [
        "If you were to apply this jet data array to a DNN, you would need to change the shape of the input data. PyTorch operations generally apply to the last dimension of an array. The data is of shape `N_events x N_jets x N_jet_features` - in other words our if we applied a DNN to this, it would only apply to the final jet!\n",
        "\n",
        "Our DNN needs to operate on an object of shape `N_events x N_DNN_features`. If we have four jets each with four features, this is 16 features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bx98etbuYMR_"
      },
      "outputs": [],
      "source": [
        "Nevents = unstructured_jet_data.shape[0]\n",
        "reshaped_jet_data = unstructured_jet_data.reshape(Nevents,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5AWB_6YMR_"
      },
      "source": [
        "We can then apply DNN operations on an array of this type e.g."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMAEhOkhYMR_"
      },
      "outputs": [],
      "source": [
        "from torch import nn, tensor\n",
        "x = tensor(reshaped_jet_data, dtype=torch.float32)\n",
        "example_linear_layer = nn.Linear(16,64)\n",
        "output = example_linear_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJpFspUyYMSA"
      },
      "source": [
        "# Bonus 2 - Other Extensions\n",
        "\n",
        "The above examples provide a means of taking ATLAS OpenData and converting it into a form for training a DNN. We then built the DNN. Here are some ideas of how to develop this.\n",
        "\n",
        "### Further DNN Ideas\n",
        "* Add early stopping to the model, such that the training will terminate early is the loss is not decreasing\n",
        "* Add a learning rate scheduler, to dynamically adjust the learning rate\n",
        "* Add regularisation in the form of dropout layers (`nn.Dropout`)\n",
        "* Split each pass of the data into batches. This becomes important for computationally expensive models, where the operations on the training data become intractable without further splitting of the data into chunks, or \"batches\".\n",
        "* Turn the model into a multi-class tagger. Find a third sample, probably a diboson sample i.e. $WW^*$ continuum production.\n",
        "\n",
        "### Understanding the model\n",
        "* With our simple DNN example, we can study the importance of each feature by randonly shuffling the values of individual features and seeing the extent to which the model degrades in performance.\n",
        "\n",
        "### Making it all more PyTorch\n",
        "PyTorch has extensive functionality; once we understand the basic structure of ML pipelines, we can then build code which better utilises the amazing functionality of pyTorch. Some ideas, in a rough order of increasing complexity:\n",
        "* Use the `dataset` and `dataloader` to define datasets and automate batching\n",
        "* Use the `nn.Module` functionality to build your DNN\n",
        "* Build a custom dataset with `torch.dataset`\n",
        "* Finally, use PyTorch-Lightning for prototyping models whilst hiding away much boilerplate code!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPQ0ovkYMSA"
      },
      "source": [
        "Here's an example DNN built using `nn.Module`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpDD5f8CYMSA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleDNN(nn.Module):\n",
        "    def __init__(self, N_input_features): # You can add more parameters here, such that the size of all layers can be\n",
        "        # defined in the constructor\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and two activation functions, assigning them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(SimpleDNN, self).__init__()\n",
        "        self.linear1 = nn.Linear(N_input_features, 50)\n",
        "        self.linear2 = nn.Linear(50, 1)\n",
        "        self.activ1 = nn.ReLU()\n",
        "        self.activ_final = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Takes as input a tensor x and returns the output tensor after applying the defined layers and activation functions.\n",
        "        \"\"\"\n",
        "        # Compute the forward pass.\n",
        "        x = self.linear1(x)\n",
        "        x = self.activ1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activ_final(x)\n",
        "        return x"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CDcHHYC4BDn7",
        "aTfdwWpEYMRz",
        "ZqpULlZsYMR-",
        "16lj5fEHYMR-",
        "3xpr5j06s6A0",
        "xe99QykGYMR_",
        "pJpFspUyYMSA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
